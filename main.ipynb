{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "from matplotlib import pyplot as plt\n",
    "from preprocess import *\n",
    "from sampling import *\n",
    "from numpy import fft as fft\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "records = wfdb.get_record_list('mitdb')\n",
    "directory = 'data/'\n",
    "sample_rate = 360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store signals, annotation positions and annotation symbols in dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal data\n",
    "\n",
    "SignalData = {}\n",
    "\n",
    "for record in records:    \n",
    "    signal, field = wfdb.rdsamp(directory + record)\n",
    "    for channel in [0,1]:\n",
    "        if field['sig_name'][channel] == 'MLII': # only use data from MLII\n",
    "            SignalData[record + ' ' + field['sig_name'][channel]] = signal[:, channel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# annotation data\n",
    "\n",
    "AnnPos, AnnSym = {}, {}\n",
    "\n",
    "for record in records:\n",
    "    annotation = wfdb.rdann(directory + record, 'atr')\n",
    "    AnnPos[record] = annotation.sample\n",
    "    AnnSym[record] = annotation.symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute segmentation positions\n",
    "Segmentation positions are the middle points between 2 neighboring beats. They serve as the starting positions of the following beats. For the first beat in a record, the starting position is set to be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SegPos = segmentation(AnnPos) # Segpos is a dictionary that contains all the segmentation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove noise from the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in SignalData.keys():\n",
    "    \n",
    "    SignalData[key] = denoise(SignalData[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw samples from the data with a rolling window.\n",
    "\n",
    "Each sample contains:\n",
    "\n",
    "1. signals collected in a feature derivation window (FDW)\n",
    "2. beat annotations in a forecast window (FW)\n",
    "And there is a time gap between FDW and FW.\n",
    "The goal is to use 1 to predict 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some parameters\n",
    "FDW_width, FW_width, gap_width = 25, 1, 0  # widths(in number of beats) of the feature derivation window, forecast window, and the gap in between.\n",
    "delay = 1 # distance(in number of beats) between two consequential FDWs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSet, LabelSet = rolling_window(SignalData, SegPos, AnnSym, FDW_width, FW_width, gap_width, delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples is 106902\n",
      "Minimum FDW in the samples contains 2215 data points\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples is %d' %(len(DataSet)))\n",
    "print('Minimum FDW in the samples contains ' + str(min(len(value) for value in DataSet.values())) + ' data points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last column in each array in DataSet is the patient information\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ -6.71225956e-02,  -6.75534839e-02,  -6.75534839e-02, ...,\n",
       "         2.52767827e-02,   2.39828495e-02,   1.00000000e+02])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The last column in each array in DataSet is the patient information')\n",
    "DataSet[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the data to be the same lengths: <br>\n",
    "    Method 1: just crop out the beginning part of the arrays that is longer than the minimum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cropping\n",
    "for key in DataSet.keys():\n",
    "    DataSet[key] = DataSet[key][-length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose only forecast windows that contains an N or L beat,  downsample N beat, and build X/y matrix as machine learning input/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73942, 26385)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return keys of N and L beats in the dataset\n",
    "N_keys = [key for key, value in LabelSet.items() if value == ['N']]\n",
    "Ab_keys = [key for key, value in LabelSet.items() if value[0] in [\n",
    "    'L','R','B','A','a','J','S','V','r','F','e','j','n','E','P','f','Q']]\n",
    "# count numbers of N and L beats in the forecast window\n",
    "nN, nAb = len(N_keys), len(Ab_keys)\n",
    "nN, nAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build X and y  (note that the last column in X is patient information)\n",
    "X = np.zeros((nAb * 2, length))\n",
    "y = np.zeros(nAb * 2)\n",
    "\n",
    "N_keys_reduced = shuffle(N_keys, n_samples = nAb, random_state = 0) # downsample N beat samples.\n",
    "\n",
    "i = 0\n",
    "for key in N_keys_reduced: \n",
    "    y[i] = 0\n",
    "    X[i,:] = DataSet[key]\n",
    "    i = i + 1\n",
    "for key in Ab_keys:\n",
    "    y[i] = 1\n",
    "    X[i,:] = DataSet[key]\n",
    "    i = i + 1\n",
    "X, y = shuffle(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xf = np.zeros((X.shape[0], X.shape[1] + 1))\n",
    "for i in range(X.shape[0]):\n",
    "    xf = fft.rfft(X[i,:-1]) # remove the last number in each array that represent patient information.\n",
    "    Xf[i, 0:(int((X.shape[1] + 1)/2))] = np.real(xf)\n",
    "    Xf[i, (int((X.shape[1] + 1)/2)):] = np.imag(xf)\n",
    "Xf = np.append(Xf, X[:,-1][:,None], axis = 1) # add patient information to the last column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_split, y_split = split_dataset_by_patient(Xf, y, [0.8, 0.2])\n",
    "X_train, X_test = X_split[0], X_split[1]\n",
    "y_train, y_test = y_split[0], y_split[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_nnw = Pipeline([('scl', StandardScaler()), ('pca', PCA(n_components = 200)),\n",
    "                     ('clf', MLPClassifier(hidden_layer_sizes = (200,), solver = 'adam', \n",
    "                                           random_state = 0, max_iter = 200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_partition, y_partition = split_dataset_by_patient(X_train, y_train, [1/3, 1/3, 1/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score, valid_score = [0, 0, 0], [0, 0, 0]\n",
    "for i in range(3):\n",
    "    \n",
    "    valid_part = i\n",
    "    train_part = [part for part in range(3) if part != valid_part]\n",
    "    X_train_part = np.concatenate((X_partition[train_part[0]], X_partition[train_part[1]]), axis = 0)[:, :-1]\n",
    "    X_valid_part = X_partition[valid_part][:, :-1]\n",
    "    y_train_part = np.concatenate((y_partition[train_part[0]], y_partition[train_part[1]]))\n",
    "    y_valid_part = y_partition[valid_part]\n",
    "    \n",
    "    pipe_nnw.fit(X_train_part, y_train_part)\n",
    "    train_score[i] = pipe_nnw.score(X_train_part, y_train_part)\n",
    "    valid_score[i] = pipe_nnw.score(X_valid_part, y_valid_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
